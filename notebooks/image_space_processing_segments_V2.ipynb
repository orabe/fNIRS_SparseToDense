{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0cdd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "09 \n",
    "import torch\n",
    "import pickle\n",
    "import cedalion\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import cedalion.sigproc.motion_correct as motion_correct\n",
    "import cedalion.sigproc.quality as quality\n",
    "import cedalion.sigproc.physio as physio\n",
    "from cedalion.io.forward_model import load_Adot,save_Adot\n",
    "import cedalion.dot as dot\n",
    "from cedalion import units\n",
    "\n",
    "import cedalion.nirs as nirs\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def get_bad_ch_mask(int_data):\n",
    "    # Saturated and Dark Channels\n",
    "\n",
    "    dark_sat_thresh = [1e-3, 0.84]\n",
    "    amp_threshs_sat = [0., dark_sat_thresh[1]]\n",
    "    amp_threshs_low = [dark_sat_thresh[0], 1]\n",
    "    _, amp_mask_sat = quality.mean_amp(int_data, amp_threshs_sat)\n",
    "    _, amp_mask_low = quality.mean_amp(int_data, amp_threshs_low)\n",
    "    _, snr_mask = quality.snr(int_data, 10)\n",
    "    amp_mask=amp_mask_sat & amp_mask_low\n",
    "\n",
    "    _, list_bad_ch = quality.prune_ch(int_data, [amp_mask, snr_mask], \"all\")\n",
    "   \n",
    "    return list_bad_ch\n",
    "\n",
    "base_dir = \"/home/\"\n",
    "# processed_data = os.path.join(base_dir, \"data/yuanyuan_v2_processed_partial\") ## Yuanyuan Dataset\n",
    "processed_data = os.path.join(base_dir, \"data/combined_fnirs_partial_head\") ## Laura Dataset\n",
    "if not os.path.exists(processed_data):\n",
    "    os.makedirs(processed_data)\n",
    "dataset_path = os.path.join(base_dir, \"data/BallSqueezingHD_modified\") ## Yuanyuan Dataset\n",
    "# dataset_path = os.path.join(base_dir, \"data/BS_Laura\") ## Laura Dataset\n",
    "\n",
    "sensitivity_fname = os.path.join(\"/data/sensitivity_yuanyuan_bs_v2.h5\") ## Yuanyuan sensitivity\n",
    "# sensitivity_fname = os.path.join(\"/data/sensitivity_laura_bs_v2.h5\") ## Laura sensitivity\n",
    "\n",
    "sensitive_parcels_path = os.path.join(base_dir, \"data/sensitive_parcels_fh.pkl\")\n",
    "keep_parcels_path = os.path.join(base_dir, \"data/sensitive_parcels.pkl\")\n",
    "\n",
    "# # Get all snirf files\n",
    "import sys\n",
    "sub = sys.argv[1]\n",
    "\n",
    "print(\"processing subject: \", sub)\n",
    "all_files = glob.glob(os.path.join(dataset_path, sub) + \"/**/*.snirf\", recursive=True)\n",
    "\n",
    "with open(sensitive_parcels_path, \"rb\") as f:\n",
    "    sensitive_parcels = pickle.load(f)\n",
    "with open(keep_parcels_path, \"rb\") as f:\n",
    "    keep_parcels = pickle.load(f)\n",
    "\n",
    "Adot = load_Adot(sensitivity_fname)\n",
    "recon = dot.ImageRecon(\n",
    "    Adot,\n",
    "    recon_mode=\"mua2conc\",\n",
    "    brain_only=True,\n",
    "    alpha_meas=10,\n",
    "    alpha_spatial=10e-3,\n",
    "    apply_c_meas=True,\n",
    "    spatial_basis_functions=None,\n",
    ")\n",
    "\n",
    "for file in all_files:\n",
    "    records = cedalion.io.read_snirf(file)\n",
    "    rec = records[0]\n",
    "\n",
    "    rec.stim = rec.stim.sort_values(by=\"onset\") ## Yuanyuan dataset\n",
    "\n",
    "    rec['rep_amp'] = quality.repair_amp(rec['amp'], median_len=3, method='linear')  # Repair Amp\n",
    "    rec['od_amp'], baseline= nirs.cw.int2od(rec['rep_amp'],return_baseline=True)\n",
    "\n",
    "    # motion correct [TDDR + WAVELET]\n",
    "    rec[\"od_tddr\"] = motion_correct.tddr(rec[\"od_amp\"])\n",
    "    rec[\"od_tddr_wavel\"] = motion_correct.wavelet(rec[\"od_tddr\"])\n",
    "\n",
    "    #-----------------------------------------highpass filter--------------------------------\n",
    "    rec['od_hpfilt'] = rec['od_tddr_wavel'].cd.freq_filter(fmin=0.008,fmax=0,butter_order=4)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # clean amplitude data\n",
    "    rec['amp_clean'] = cedalion.nirs.cw.od2int(rec['od_hpfilt'], baseline)\n",
    "\n",
    "    # get bad channel mask\n",
    "    list_bad_ch = get_bad_ch_mask(rec[\"amp_clean\"]) # this has custom paramerers!? \n",
    "    print('the list of bad channels: ', len(list_bad_ch))\n",
    "\n",
    "    # channel variance\n",
    "    od_var_vec = quality.measurement_variance(rec[\"od_hpfilt\"], list_bad_channels=list_bad_ch, bad_rel_var=1e6,calc_covariance=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    dpf = xr.DataArray(\n",
    "        [6, 6],\n",
    "        dims=\"wavelength\",\n",
    "        coords={\"wavelength\": rec[\"amp\"].wavelength},\n",
    "    )\n",
    "    rec['conc'] = cedalion.nirs.cw.od2conc(rec['od_hpfilt'], rec.geo3d, dpf, spectrum=\"prahl\")\n",
    "\n",
    "    # conc_pr vs conc \n",
    "    chromo_var = quality.measurement_variance(rec['conc'], list_bad_channels = list_bad_ch, bad_rel_var = 1e6, calc_covariance = False)\n",
    "    rec['conc_pcr'], gb_comp_rem = physio.global_component_subtract(rec['conc'],ts_weights=1/chromo_var,k=0,spatial_dim='channel',spectral_dim='chromo')\n",
    "\n",
    "    rec['od_pcr1'] = cedalion.nirs.cw.conc2od(rec['conc_pcr'], rec.geo3d, dpf, spectrum=\"prahl\")#     delta_conc = chunked_eff_xr_matmult(od_stacked, B, contract_dim=\"flat_channel\", sample_dim=\"time\", chunksize=300)\n",
    "    c_meas = quality.measurement_variance(rec['od_hpfilt'], list_bad_channels=list_bad_ch, bad_rel_var=1e6,calc_covariance=False)\n",
    "\n",
    "    delta_conc = recon.reconstruct(rec['od_pcr1'], c_meas) \n",
    "    delta_conc.time.attrs[\"units\"] = units.s\n",
    "\n",
    "    dC_brain = delta_conc.cd.freq_filter(fmin=0.01, fmax=0.5, butter_order=4)\n",
    "    dC_brain = dC_brain.sel(time=slice(rec.stim.onset.values[0]-3 , rec.stim.onset.values[-1]+13))\n",
    "    dC_brain = dC_brain.where(dC_brain.is_brain == True)\n",
    "    # alternatively use 1/conc_var to weight vertex sensitivity and then normalize by sum of weights\n",
    "    dC_brain = dC_brain.pint.quantify().pint.to(\"uM\").pint.dequantify()\n",
    "\n",
    "    hbr = dC_brain.sel(chromo='HbR').groupby('parcel').mean()\n",
    "    hbo = dC_brain.sel(chromo='HbO').groupby('parcel').mean()\n",
    "    signal_raw = xr.concat([hbo, hbr], dim='chromo')\n",
    "\n",
    "    # revised matrix\n",
    "    signal_raw = signal_raw.sel(parcel=signal_raw.parcel != 'Background+FreeSurfer_Defined_Medial_Wall_LH')\n",
    "    signal_raw = signal_raw.sel(parcel=signal_raw.parcel != 'Background+FreeSurfer_Defined_Medial_Wall_RH')\n",
    "    \n",
    "    delta_conc, global_comp = physio.global_component_subtract(signal_raw, ts_weights=None, k=0, \n",
    "                                                        spatial_dim='parcel', spectral_dim= 'chromo')\n",
    "\n",
    "    delta_conc = delta_conc / np.abs(delta_conc).max()\n",
    "    delta_conc = delta_conc.fillna(0)\n",
    "    delta_conc = delta_conc.transpose(\"time\", \"parcel\", \"chromo\")\n",
    "    delta_brain = delta_conc.copy()\n",
    "\n",
    "    # create a boolean mask along 'parcel' dimension\n",
    "    delta_brain = delta_brain.sel(parcel=keep_parcels)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    for index, row in rec.stim.iterrows():\n",
    "        label = row[\"trial_type\"]\n",
    "        duration = 10\n",
    "        start_list = np.linspace(-2.5, 2.5, 9)\n",
    "        for s in start_list:\n",
    "            start_time = row[\"onset\"] + s\n",
    "            end_time = start_time + duration + 5   # in seconds\n",
    "            baseline = delta_brain.sel(time=slice(row[\"onset\"] - 2.5 , row[\"onset\"])).mean(\"time\")\n",
    "            # Then, trimming is easy with `.sel()`:\n",
    "            x = delta_brain.sel(time=slice(start_time, end_time)) - baseline\n",
    "            x = x.isel(time=slice(0, 87))\n",
    "            x = x.transpose(\"parcel\", \"chromo\", \"time\")\n",
    "            del x.time.attrs['units']\n",
    "            if not os.path.exists(os.path.dirname(file.replace(dataset_path, processed_data))):\n",
    "                os.makedirs(os.path.dirname(file.replace(dataset_path, processed_data)))\n",
    "            if s == 0:\n",
    "                x.to_netcdf(file.replace(dataset_path, processed_data).replace(\".snirf\", \"_\" + label + \"_\"+str(i)+\"_test.nc\"))\n",
    "                i += 1\n",
    "            else:\n",
    "                x.to_netcdf(file.replace(dataset_path, processed_data).replace(\".snirf\", \"_\" + label + \"_\"+str(i)+\".nc\"))\n",
    "                i += 1\n",
    "    print(\"finished processing file: \", os.path.basename(file).replace(\".snirf\",\".npy\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
