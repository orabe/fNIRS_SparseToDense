{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0a13d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 20 17:35:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:B1:00.0 Off |                  N/A |\n",
      "|  0%   26C    P8             14W /  350W |       1MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5539e35",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cedalion.sigproc.motion_correct'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcedalion\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcedalion\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msigproc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmotion_correct\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmotion_correct\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcedalion\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msigproc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquality\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mquality\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcedalion\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msigproc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mphysio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mphysio\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cedalion.sigproc.motion_correct'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cedalion\n",
    "import numpy as np\n",
    "import cedalion.sigproc.motion_correct as motion_correct\n",
    "import cedalion.sigproc.quality as quality\n",
    "import cedalion.sigproc.physio as physio\n",
    "from cedalion.io.forward_model import load_Adot,save_Adot\n",
    "import cedalion.dot as dot\n",
    "from cedalion import units\n",
    "\n",
    "import cedalion.nirs as nirs\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path, PureWindowsPath\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e135c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_ch_mask(int_data):\n",
    "    # Saturated and Dark Channels\n",
    "\n",
    "    dark_sat_thresh = [1e-3, 0.84]\n",
    "    amp_threshs_sat = [0., dark_sat_thresh[1]]\n",
    "    amp_threshs_low = [dark_sat_thresh[0], 1]\n",
    "    _, amp_mask_sat = quality.mean_amp(int_data, amp_threshs_sat)\n",
    "    _, amp_mask_low = quality.mean_amp(int_data, amp_threshs_low)\n",
    "    _, snr_mask = quality.snr(int_data, 10)\n",
    "    amp_mask=amp_mask_sat & amp_mask_low\n",
    "\n",
    "    _, list_bad_ch = quality.prune_ch(int_data, [amp_mask, snr_mask], \"all\")\n",
    "   \n",
    "    return list_bad_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edca5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/orabe/fNIRS_sparseToDense/\"\n",
    "DATASET_NAME = \"BallSqueezingHD_modified\"\n",
    "\n",
    "raw_path = Path(f'datasets/raw/{DATASET_NAME}')\n",
    "\n",
    "pre_processed_path = Path(f'datasets/pre_processed/{DATASET_NAME}')\n",
    "pre_processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/orabe/fNIRS_sparseToDense'\n",
    "\n",
    "# Available datasets:\n",
    "# DATASET_NAME = \"BallSqueezingHD_modified\"\n",
    "DATASET_NAME = \"FreshMotor\"\n",
    "# DATASET_NAME = \"BS_Laura\"\n",
    "# DATASET_NAME = \"ElectricalThermal\"\n",
    "\n",
    "\n",
    "if DATASET_NAME == \"BallSqueezingHD_modified\":\n",
    "    raw_path = f\"{raw_path}/{DATASET_NAME}/sub-*/nirs/sub-*.snirf\"\n",
    "\n",
    "elif DATASET_NAME == \"BS_Laura\":\n",
    "    raw_path = f\"{raw_path}/{DATASET_NAME}/sub-*/nirs/sub-*.snirf\"\n",
    "    \n",
    "elif DATASET_NAME == \"Electrical_Thermal\":\n",
    "    raw_path = f\"{raw_path}/{DATASET_NAME}/sub-*/ses-*/nirs/sub-*_ses-*_task-Electrical*_nirs.snirf\"\n",
    "    # TODO: exclude subjects without txt files for landmarks coords\n",
    "    \n",
    "elif DATASET_NAME == \"FreshMotor\":\n",
    "    duration = \"*\" # * to include both 2s and 3s\n",
    "    raw_path = f\"{raw_path}/{DATASET_NAME}/sub-*/ses-*{duration}/nirs/sub-*_ses-*{duration}_task-FRESHMOTOR_nirs.snirf\"\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset name\")\n",
    "\n",
    "files = glob.glob(raw_path)\n",
    "\n",
    "# TODO: to be confirmed\n",
    "# remove non-BS files for Laura's dataset to avoid errors\n",
    "if DATASET_NAME == \"BS_Laura\":\n",
    "    files = [p for p in files if \"BS\" in os.path.basename(p)]\n",
    "    \n",
    "files = sorted(files)\n",
    "print(f\"{len(files)} files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = files[0] # select one\n",
    "rec = cedalion.io.read_snirf(filename)[0]  # read snirf files\n",
    "meas_list = rec._measurement_lists[\"amp\"]\n",
    "\n",
    "head_icbm152 = dot.get_standard_headmodel('icbm152')  \n",
    "geo3d_snapped_ijk = head_icbm152.align_and_snap_to_scalp(rec.geo3d)\n",
    "\n",
    "fwm = cedalion.dot.forward_model.ForwardModel(\n",
    "    head_icbm152, \n",
    "    geo3d_snapped_ijk,\n",
    "    meas_list\n",
    ")\n",
    "\n",
    "fluence_fname = os.path.join(pre_processed_path, \"fluence_\" + DATASET_NAME + \".h5\")\n",
    "sensitivity_fname = os.path.join(pre_processed_path, \"sensitivity_\" + DATASET_NAME + \".h5\")\n",
    "\n",
    "fwm.compute_fluence_mcx(fluence_fname)\n",
    "fwm.compute_sensitivity(fluence_fname, sensitivity_fname)\n",
    "\n",
    "Adot = load_Adot(sensitivity_fname)\n",
    "recon = dot.ImageRecon(\n",
    "    Adot,\n",
    "    recon_mode=\"mua2conc\",\n",
    "    brain_only=True,\n",
    "    alpha_meas=10,\n",
    "    alpha_spatial=10e-3,\n",
    "    apply_c_meas=True,\n",
    "    spatial_basis_functions=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_to_rec = {}\n",
    "skipped_subjects = []\n",
    "\n",
    "for f in files:\n",
    "    records = cedalion.io.read_snirf(f)\n",
    "    rec = records[0]\n",
    "\n",
    "    rec.stim = rec.stim.sort_values(by=\"onset\") ## Yuanyuan dataset\n",
    "\n",
    "    rec['rep_amp'] = quality.repair_amp(rec['amp'], median_len=3, method='linear')  # Repair Amp\n",
    "    rec['od_amp'], baseline= nirs.cw.int2od(rec['rep_amp'],return_baseline=True)\n",
    "\n",
    "    # motion correct [TDDR + WAVELET]\n",
    "    rec[\"od_tddr\"] = motion_correct.tddr(rec[\"od_amp\"])\n",
    "    rec[\"od_tddr_wavel\"] = motion_correct.wavelet(rec[\"od_tddr\"])\n",
    "\n",
    "    #-----------------------------------------highpass filter--------------------------------\n",
    "    rec['od_hpfilt'] = rec['od_tddr_wavel'].cd.freq_filter(fmin=0.008,fmax=0,butter_order=4)\n",
    "    #----------------------------------------------------------------------------------------\n",
    "\n",
    "    # clean amplitude data\n",
    "    rec['amp_clean'] = cedalion.nirs.cw.od2int(rec['od_hpfilt'], baseline)\n",
    "\n",
    "    # get bad channel mask\n",
    "    list_bad_ch = get_bad_ch_mask(rec[\"amp_clean\"]) # this has custom paramerers!? \n",
    "    print('the list of bad channels: ', len(list_bad_ch))\n",
    "\n",
    "    # channel variance\n",
    "    od_var_vec = quality.measurement_variance(rec[\"od_hpfilt\"], list_bad_channels=list_bad_ch, bad_rel_var=1e6,calc_covariance=False)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    dpf = xr.DataArray(\n",
    "        [6, 6],\n",
    "        dims=\"wavelength\",\n",
    "        coords={\"wavelength\": rec[\"amp\"].wavelength},\n",
    "    )\n",
    "    rec['conc'] = cedalion.nirs.cw.od2conc(rec['od_hpfilt'], rec.geo3d, dpf, spectrum=\"prahl\")\n",
    "\n",
    "    # conc_pr vs conc \n",
    "    chromo_var = quality.measurement_variance(rec['conc'], list_bad_channels = list_bad_ch, bad_rel_var = 1e6, calc_covariance = False)\n",
    "    rec['conc_pcr'], gb_comp_rem = physio.global_component_subtract(rec['conc'],ts_weights=1/chromo_var,k=0,spatial_dim='channel',spectral_dim='chromo')\n",
    "\n",
    "    rec['od_pcr1'] = cedalion.nirs.cw.conc2od(rec['conc_pcr'], rec.geo3d, dpf, spectrum=\"prahl\")#     delta_conc = chunked_eff_xr_matmult(od_stacked, B, contract_dim=\"flat_channel\", sample_dim=\"time\", chunksize=300)\n",
    "    c_meas = quality.measurement_variance(rec['od_hpfilt'], list_bad_channels=list_bad_ch, bad_rel_var=1e6,calc_covariance=False)\n",
    "\n",
    "    delta_conc = recon.reconstruct(rec['od_pcr1'], c_meas) \n",
    "    delta_conc.time.attrs[\"units\"] = units.s\n",
    "\n",
    "    dC_brain = delta_conc.cd.freq_filter(fmin=0.01, fmax=0.5, butter_order=4)\n",
    "    dC_brain = dC_brain.sel(time=slice(rec.stim.onset.values[0]-3 , rec.stim.onset.values[-1]+13))\n",
    "    dC_brain = dC_brain.where(dC_brain.is_brain == True)\n",
    "    # alternatively use 1/conc_var to weight vertex sensitivity and then normalize by sum of weights\n",
    "    dC_brain = dC_brain.pint.quantify().pint.to(\"uM\").pint.dequantify()\n",
    "\n",
    "    hbr = dC_brain.sel(chromo='HbR').groupby('parcel').mean()\n",
    "    hbo = dC_brain.sel(chromo='HbO').groupby('parcel').mean()\n",
    "    signal_raw = xr.concat([hbo, hbr], dim='chromo')\n",
    "\n",
    "    # revised matrix\n",
    "    signal_raw = signal_raw.sel(parcel=signal_raw.parcel != 'Background+FreeSurfer_Defined_Medial_Wall_LH')\n",
    "    signal_raw = signal_raw.sel(parcel=signal_raw.parcel != 'Background+FreeSurfer_Defined_Medial_Wall_RH')\n",
    "    \n",
    "    delta_conc, global_comp = physio.global_component_subtract(\n",
    "        signal_raw, \n",
    "        ts_weights=None, k=0, \n",
    "        spatial_dim='parcel',\n",
    "        spectral_dim= 'chromo')\n",
    "\n",
    "    delta_conc = delta_conc / np.abs(delta_conc).max()\n",
    "    delta_conc = delta_conc.fillna(0)\n",
    "    delta_conc = delta_conc.transpose(\"time\", \"parcel\", \"chromo\")\n",
    "\n",
    "    parcel_dOD, parcel_mask = fwm.parcel_sensitivity(\n",
    "        Adot,\n",
    "        list_bad_ch,\n",
    "        dOD_thresh = 0.001,       \n",
    "        minCh=1,\n",
    "        dHbO=10,\n",
    "        dHbR=-3\n",
    "    )\n",
    "    sensitive_parcels = parcel_mask.where(parcel_mask, drop=True)[\"parcel\"].values.tolist()\n",
    "    dropped_parcels = parcel_mask.where(~parcel_mask, drop=True)[\"parcel\"].values.tolist()\n",
    "    print(f\"Number of sensitive parcels: {len(sensitive_parcels)}\")\n",
    "    print(f\"Number of dropped parcels: {len(dropped_parcels)}\")\n",
    "    \n",
    "    data = {\n",
    "        'delta_conc': delta_conc,\n",
    "        'dropped_parcels': dropped_parcels,\n",
    "        'sensitive_parcels': sensitive_parcels,\n",
    "    }\n",
    "    \n",
    "    # save as pickle\n",
    "    path = PureWindowsPath(f)\n",
    "    subject_dir = path.parts[-3]\n",
    "    filename = path.stem\n",
    "\n",
    "    if DATASET_NAME == \"FreshMotor\":\n",
    "        subject_dir = path.parts[-4]\n",
    "        session_label = path.parts[-3]\n",
    "        task_fragment = next(\n",
    "            (part for part in filename.split('_') if part.startswith('task-')),\n",
    "            f\"task-{DATASET_NAME.replace('_', '').upper()}\",\n",
    "        )\n",
    "        run_fragment = session_label.replace('ses-', 'run-')\n",
    "        filename = f'{subject_dir}_{task_fragment}_{run_fragment}_nirs'\n",
    "\n",
    "    if subject_dir not in subject_to_rec:\n",
    "        subject_to_rec[subject_dir] = []\n",
    "\n",
    "    all_parcels_dir = pre_processed_path / 'ts_all_parcels' / subject_dir\n",
    "    all_parcels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_name_to_save = all_parcels_dir / f'{filename}_ts_all_parcels.pkl'\n",
    "\n",
    "    with open(file_name_to_save, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "if skipped_subjects:\n",
    "    print(f\"Skipped {len(skipped_subjects)} file(s) because all channels were bad.\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET_NAME == \"BallSqueezingHD_modified\":\n",
    "    MAX_PARCELS = 0\n",
    "    PARCEL_TEMPLATE = None\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'rb') as handle:\n",
    "            data_pickle = pickle.load(handle)        \n",
    "        timeseries = data_pickle['delta_conc']\n",
    "        sensitive_parcels = data_pickle['sensitive_parcels']\n",
    "        \n",
    "        sensitive_timeseries = timeseries.sel(parcel=sensitive_parcels)\n",
    "        \n",
    "        if sensitive_timeseries.sizes[\"parcel\"] > MAX_PARCELS:\n",
    "            MAX_PARCELS = sensitive_timeseries.sizes[\"parcel\"]\n",
    "            PARCEL_TEMPLATE = sensitive_timeseries.parcel.values.tolist()\n",
    "    \n",
    "    print(f\"Max parcels across subjects: {MAX_PARCELS}\")\n",
    "    print(f\"Parcel template: {PARCEL_TEMPLATE}\")\n",
    "\n",
    "    # Save the template parcels to pkl file\n",
    "    with open(f'{pre_processed_path}/parcel_template_{DATASET_NAME}.pkl', 'wb') as handle:\n",
    "        pickle.dump(PARCEL_TEMPLATE, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any dataset: load Pre-defined template parcel from BallSqueezingHD_modified file\n",
    "with open(f'{pre_processed_path.parents[0]}/BallSqueezingHD_modified/parcel_template_BallSqueezingHD_modified.pkl', 'rb') as handle:\n",
    "    PARCEL_TEMPLATE = pickle.load(handle)\n",
    "len(PARCEL_TEMPLATE)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ba16c",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (all) parcel files\n",
    "preproc_files_path = str(pre_processed_path / 'ts_all_parcels' / 'sub-*' / '*.pkl')\n",
    "pkl_files = glob.glob(preproc_files_path)\n",
    "\n",
    "len(pkl_files), pkl_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_path = Path(f'datasets/processed/{DATASET_NAME}')\n",
    "processed_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a77c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_duration = 2.5  # in seconds\n",
    "n_shifts = 9\n",
    "duration = 10  # in seconds\n",
    "post_padding = 5  # in seconds\n",
    "n_timepoints = 87  # fixed length after shifting\n",
    "\n",
    "if DATASET_NAME == \"BallSqueezingHD_modified\":\n",
    "    delta_range = (-2.5, 2.5)\n",
    "elif DATASET_NAME == \"FreshMotor\":\n",
    "    delta_range = (-2.0, 0.0)\n",
    "start_shift = np.linspace(*delta_range, n_shifts)\n",
    "\n",
    "\n",
    "for file in pkl_files:\n",
    "    with open(file, 'rb') as handle:\n",
    "        data_pickle = pickle.load(handle)\n",
    "    \n",
    "    delta_brain = data_pickle['delta_conc']\n",
    "    sensitive_parcels = data_pickle['sensitive_parcels']\n",
    "\n",
    "    # Align subject-specific parcels to a common parcel template (zero-pad missing parcels)\n",
    "    delta_brain = delta_brain.sel(parcel=sensitive_parcels).reindex(parcel=PARCEL_TEMPLATE, fill_value=0)\n",
    "\n",
    "    i = 0\n",
    "    for index, row in rec.stim.iterrows():\n",
    "        label = row[\"trial_type\"].lower()\n",
    "        for s in start_shift:\n",
    "            start_time = row[\"onset\"] + s\n",
    "            end_time = start_time + duration + post_padding # in seconds\n",
    "            baseline = delta_brain.sel(\n",
    "                time=slice(row[\"onset\"] - baseline_duration, row[\"onset\"])\n",
    "            ).mean(\"time\")\n",
    "            \n",
    "            # Then, trimming is easy with `.sel()`:\n",
    "            x = delta_brain.sel(time=slice(start_time, end_time)) - baseline\n",
    "            x = x.isel(time=slice(0, n_timepoints))\n",
    "            x = x.transpose(\"parcel\", \"chromo\", \"time\")\n",
    "            del x.time.attrs['units']\n",
    "\n",
    "            if s == 0:\n",
    "                x.to_netcdf(file.replace(processed_path).replace(\".snirf\", \"_\" + label + \"_\"+str(i)+\"_test.nc\"))\n",
    "                i += 1\n",
    "            else:\n",
    "                x.to_netcdf(file.replace(processed_path).replace(\".snirf\", \"_\" + label + \"_\"+str(i)+\".nc\"))\n",
    "                i += 1\n",
    "    print(\"finished processing file: \", os.path.basename(file).replace(\".snirf\",\".npy\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
