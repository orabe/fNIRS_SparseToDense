{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b87097",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from pathlib import PureWindowsPath\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CNN2D_BaselineV2(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Conv2d(100, 64, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(16*1*2, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x = x.permute(0, 2, 1, 3)\n",
    "        return self.model(x)\n",
    "\n",
    "class CNN2D_BaselineV21(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Conv2d(104, 64, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(16*1*2, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class CNN2D_BaselineV3(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Conv2d(100, 64, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(16*1*1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class CNN2D_Baseline_Image(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Conv2d(104, 64, kernel_size=(1, 3)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "\n",
    "            # nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(16*1*3, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class CNN2D_Baseline_Image_both_chromos(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.model = nn.Sequential(\n",
    "            nn.Conv2d(104, 64, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            # nn.Dropout(0.6),\n",
    "            # nn.InstanceNorm2d(16),\n",
    "            # nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(16*2*3, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class CNN2DChannel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(100, 64, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        )\n",
    "\n",
    "        # Don't define Linear layers yet\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        if self.classifier is None:\n",
    "            # First pass — define classifier based on actual input\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(flattened_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(64, 2)\n",
    "            )\n",
    "            # Move to same device as input\n",
    "            self.classifier.to(x.device)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class CNN2DChannelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(100, 64, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        )\n",
    "\n",
    "        # Don't define Linear layers yet\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        if self.classifier is None:\n",
    "            # First pass — define classifier based on actual input\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(flattened_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(64, 2)\n",
    "            )\n",
    "            # Move to same device as input\n",
    "            self.classifier.to(x.device)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN2DImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(104, 64, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        )\n",
    "\n",
    "        # Don't define Linear layers yet\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        if self.classifier is None:\n",
    "            # First pass — define classifier based on actual input\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(flattened_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(64, 2)\n",
    "            )\n",
    "            # Move to same device as input\n",
    "            self.classifier.to(x.device)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class CNN2DImageWUSTL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(371, 64, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            # nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.6),\n",
    "            # nn.InstanceNorm2d(16),\n",
    "            # nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        )\n",
    "\n",
    "        # Don't define Linear layers yet\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        if self.classifier is None:\n",
    "            # First pass — define classifier based on actual input\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(flattened_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(64, 2)\n",
    "            )\n",
    "            # Move to same device as input\n",
    "            self.classifier.to(x.device)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class MultiStageModel(nn.Module):\n",
    "    def __init__(self, num_stages, num_layers, num_f_maps, dim, num_classes):\n",
    "        super(MultiStageModel, self).__init__()\n",
    "        self.stage1 = SingleStageModel(num_layers, num_f_maps, dim, num_classes)\n",
    "        self.stages = nn.ModuleList([copy.deepcopy(SingleStageModel(num_layers, num_f_maps, num_classes, num_classes)) for s in range(num_stages-1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stage1(x)\n",
    "        outputs = out.unsqueeze(0)\n",
    "        for s in self.stages:\n",
    "            out = s(F.softmax(out, dim=1))\n",
    "            outputs = torch.cat((outputs, out.unsqueeze(0)), dim=0)\n",
    "        return outputs\n",
    "\n",
    "class SingleStageModel(nn.Module):\n",
    "    def __init__(self, num_layers, num_f_maps, dim, num_classes):\n",
    "        super(SingleStageModel, self).__init__()\n",
    "        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(DilatedResidualLayer(2 ** i, num_f_maps, num_f_maps)) for i in range(num_layers)])\n",
    "        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_1x1(x)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        out = self.conv_out(out)\n",
    "        return out\n",
    "\n",
    "class DilatedResidualLayer(nn.Module):\n",
    "    def __init__(self, dilation, in_channels, out_channels):\n",
    "        super(DilatedResidualLayer, self).__init__()\n",
    "        self.conv_dilated = nn.Conv1d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)\n",
    "        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv_dilated(x))\n",
    "        out = self.conv_1x1(out)\n",
    "        out = self.dropout(out)\n",
    "        return (x + out)\n",
    "\n",
    "class MSTCN_WRAP(nn.Module):\n",
    "    def __init__(self, num_stages=4, num_layers=8, num_f_maps=16, dim=200, num_classes=2):\n",
    "        super(MSTCN_WRAP, self).__init__()\n",
    "        self.mstn_encode = MultiStageModel(num_stages, num_layers, num_f_maps, dim, num_classes)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(num_stages*num_classes, 16, kernel_size=(3)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.MaxPool1d(kernel_size=3),\n",
    "            nn.Conv1d(16, 32, kernel_size=(3)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.MaxPool1d(kernel_size=3),\n",
    "            nn.Conv1d(32, 32, kernel_size=(3)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.MaxPool1d(kernel_size=3),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(32*2, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1, end_dim=2)\n",
    "        x = self.mstn_encode(x)\n",
    "        x = x.permute(1, 0, 2, 3).flatten(start_dim=1, end_dim=2)\n",
    "        return self.cnn(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_layers=6) -> None:\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.project = nn.Linear(200, embedding_dim)\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1, end_dim=2).permute(0, 2, 1)\n",
    "        x = self.project(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.classification(x)\n",
    "\n",
    "\n",
    "class ImprovedTransformer(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, num_layers=4, num_classes=21, time_steps=87, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        # Linear projection from parcels to embedding dim\n",
    "        self.project = nn.Linear(371, embedding_dim)\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, time_steps, embedding_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=dropout, batch_first=True, dim_feedforward=256)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classification = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (B, 1, 371, 87)\n",
    "        x = x.flatten(start_dim=1, end_dim=2).permute(0, 2, 1)  # → (B, 87, 371)\n",
    "        x = self.project(x)  # → (B, 87, 128)\n",
    "        x = x + self.positional_encoding  # add position\n",
    "        x = self.transformer(x)  # → (B, 87, 128)\n",
    "        x = x.mean(dim=1)  # average pooling over time\n",
    "        x = self.dropout(x)\n",
    "        return self.classification(x)\n",
    "\n",
    "\n",
    "class BoldT(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, num_layers=4, num_classes=21,\n",
    "                 time_steps=87, dropout=0.1, pooling=\"cls\"):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pooling in [\"cls\", \"mean\"], \"pooling must be 'cls' or 'mean'\"\n",
    "        self.pooling = pooling\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        # Linear projection from parcels to embedding dim\n",
    "        self.project = nn.Linear(371, embedding_dim)\n",
    "\n",
    "        # CLS token (if used)\n",
    "        if pooling == \"cls\":\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "            pe_len = time_steps + 1\n",
    "        else:\n",
    "            self.cls_token = None\n",
    "            pe_len = time_steps\n",
    "\n",
    "        # Learnable positional encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, pe_len, embedding_dim))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            dim_feedforward=256\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classification = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (B, 1, 371, 87)\n",
    "        x = x.flatten(start_dim=1, end_dim=2).permute(0, 2, 1)  # → (B, 87, 371)\n",
    "        x = self.project(x)                                    # → (B, 87, embedding_dim)\n",
    "\n",
    "        B = x.size(0)\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            # Expand CLS token\n",
    "            cls_tokens = self.cls_token.expand(B, -1, -1)      # (B, 1, embedding_dim)\n",
    "            # Concatenate CLS at beginning\n",
    "            x = torch.cat((cls_tokens, x), dim=1)              # (B, 88, embedding_dim)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Pooling\n",
    "        if self.pooling == \"cls\":\n",
    "            x = x[:, 0]                                        # CLS token output\n",
    "        else:\n",
    "            x = x.mean(dim=1)                                  # mean pooling\n",
    "\n",
    "        # Classification\n",
    "        x = self.dropout(x)\n",
    "        return self.classification(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
