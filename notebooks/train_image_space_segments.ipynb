{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "816639f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello hydra\n",
      "job ID: 3795443\n",
      "array job ID: None\n",
      "array task ID: None\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "# from datasets_v02 import fNIRSChannelSpaceSegmentLoad, fNIRSPreloadDataset\n",
    "import torch\n",
    "import os\n",
    "import xarray as xr\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from model import CNN2DImage, CNN2DChannelV2, CNN2D_BaselineV2\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "# from utils import create_train_test_files, create_train_test_segments, create_train_test_segments_grad\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "print(\"hello hydra\")\n",
    "print(f\"job ID: {os.getenv('SLURM_JOB_ID')}\")\n",
    "print(f\"array job ID: {os.getenv('SLURM_ARRAY_JOB_ID')}\")\n",
    "print(f\"array task ID: {os.getenv('SLURM_ARRAY_TASK_ID')}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b002c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2DImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            # nn.Conv2d(104, 64, kernel_size=(1, 3)), (shakiba's)\n",
    "            # nn.Conv2d(110, 64, kernel_size=(1, 3)), # parcel space\n",
    "            nn.Conv2d(68, 64, kernel_size=(1, 3)), # channel space (FreshMotor)\n",
    "            # nn.Conv2d(100, 64, kernel_size=(1, 3)), # channel space (BallSqueezingHD)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(64, 32, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "            nn.Conv2d(32, 16, kernel_size=(1, 3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.MaxPool2d(kernel_size=(1, 3)),\n",
    "        )\n",
    "\n",
    "        # Don't define Linear layers yet\n",
    "        self.classifier = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        if self.classifier is None:\n",
    "            # First pass â€” define classifier based on actual input\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(start_dim=1),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(flattened_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.Linear(64, 2)\n",
    "            )\n",
    "            # Move to same device as input\n",
    "            self.classifier.to(x.device)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8410f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class fNIRSPreloadDataset(Dataset):\n",
    "    def __init__(self, data_csv_path, mode=\"train\", chromo=\"HbO\"):\n",
    "        self.data_csv = pd.read_csv(data_csv_path)\n",
    "        self.mode = mode\n",
    "        self.chromo = chromo\n",
    "\n",
    "        # === Pre-load all trials into RAM ===\n",
    "        self.all_trials = []\n",
    "        self.all_labels = []\n",
    "\n",
    "        print(f\"Preloading {len(self.data_csv)} trials into memory...\")\n",
    "\n",
    "        for i, row in self.data_csv.iterrows():\n",
    "            if chromo == \"both\":\n",
    "                record = xr.open_dataarray(row[\"snirf_file\"])\n",
    "                trial_tensor = torch.tensor(record.values, dtype=torch.float32)\n",
    "            else:\n",
    "                try:\n",
    "                    record = xr.open_dataarray(row[\"snirf_file\"]).sel(chromo=chromo)\n",
    "                    current_len = record.shape[1]\n",
    "                    target_len = 87\n",
    "\n",
    "                    # only pad if shorter than target\n",
    "                    if current_len < target_len:\n",
    "                        print(\"Padding trial from length\", current_len, \"to\", target_len)\n",
    "                        pad_width = [(0, 0), (0, target_len - current_len)]\n",
    "                        record = xr.DataArray(\n",
    "                            np.pad(record.values, pad_width, mode='constant', constant_values=0),\n",
    "                            dims=record.dims,\n",
    "                            coords={\n",
    "                                record.dims[0]: record.coords[record.dims[0]].values,\n",
    "                                record.dims[1]: np.arange(target_len)\n",
    "                            }\n",
    "                        )\n",
    "                    trial_tensor = torch.tensor(record.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {row['snirf_file']}: {e}\")\n",
    "                    continue\n",
    "            label_tensor = torch.tensor(int(row[\"trial_type\"]), dtype=torch.long)\n",
    "\n",
    "            self.all_trials.append(trial_tensor)\n",
    "            self.all_labels.append(label_tensor)\n",
    "\n",
    "        print(f\"Loaded {len(self.all_trials)} trials into memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_trials)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_trials[idx], self.all_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bab186e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def create_train_test_segments(bids_path, preprocessed_path, test_subjects_list=None, test_subject_percentage=0.2, exclude_subjects=None):\n",
    "    \"\"\"\n",
    "    Create train and test files for the dataset.\n",
    "    \"\"\"\n",
    "    # Load the participants.tsv file\n",
    "    if bids_path is not None:\n",
    "        participants_df = pd.read_csv(os.path.join(bids_path, \"participants.tsv\"), sep=\"\\t\")\n",
    "    else:\n",
    "        participants = glob.glob(preprocessed_path + \"/sub-*\")\n",
    "        participants = [os.path.basename(p) for p in participants]\n",
    "        participants_df = pd.DataFrame({\"participant_id\": participants})\n",
    "\n",
    "    if exclude_subjects is not None:\n",
    "        participants_df = participants_df[~participants_df[\"participant_id\"].isin(exclude_subjects)]\n",
    "        print(f\"Excluding subjects: {exclude_subjects}\")\n",
    "\n",
    "    if test_subjects_list is not None:\n",
    "        for test_subject in test_subjects_list:\n",
    "            if test_subject not in participants_df[\"participant_id\"].values:\n",
    "                raise ValueError(f\"Test subject {test_subject} not found in participants.tsv\")\n",
    "    else:\n",
    "        # Randomly select test subjects\n",
    "        num_test_subjects = int(len(participants_df) * test_subject_percentage)\n",
    "        test_subjects = participants_df.sample(n=num_test_subjects, random_state=42)[\"participant_id\"].values\n",
    "        test_subjects_list = list(test_subjects)\n",
    "\n",
    "    train_subjects_list = [sub for sub in participants_df[\"participant_id\"].values if sub not in test_subjects_list]\n",
    "    print(f\"Number of train subjects: {len(train_subjects_list)}\")\n",
    "\n",
    "\n",
    "    test_df = pd.DataFrame()\n",
    "    train_df = pd.DataFrame()\n",
    "\n",
    "    labels = {\"Left\": 1, \"Right\": 0, \"left\": 1, \"right\": 0}\n",
    "\n",
    "    train_files =  []\n",
    "    for train_subject in train_subjects_list:\n",
    "        # train_files += glob.glob(os.path.join(preprocessed_path, train_subject, \"**\", \"*.nc\"))\n",
    "        train_files += glob.glob(os.path.join(preprocessed_path, train_subject, \"**\", \"*.nc\"), recursive=True)\n",
    "    train_labels = []\n",
    "    for f in train_files:\n",
    "        if os.path.basename(f).endswith(\"_test.nc\"):\n",
    "            train_labels.append(labels[os.path.basename(f).split(\"_\")[-3]])\n",
    "        else:\n",
    "            train_labels.append(labels[os.path.basename(f).split(\"_\")[-2]])\n",
    "    train_df = pd.DataFrame({\n",
    "        \"snirf_file\": train_files,\n",
    "        \"trial_type\": train_labels})\n",
    "    \n",
    "    test_files =  []\n",
    "    for test_subject in test_subjects_list:\n",
    "        # test_files += glob.glob(os.path.join(preprocessed_path, test_subject, \"**\", \"*_test.nc\"))\n",
    "        test_files += glob.glob(os.path.join(preprocessed_path, test_subject, \"**\", \"*_test.nc\"), recursive=True)\n",
    "    test_labels = []\n",
    "    for f in test_files:\n",
    "        if os.path.basename(f).endswith(\"_test.nc\"):\n",
    "            test_labels.append(labels[os.path.basename(f).split(\"_\")[-3]])\n",
    "        else:\n",
    "            test_labels.append(labels[os.path.basename(f).split(\"_\")[-2]])\n",
    "    test_df = pd.DataFrame({\n",
    "        \"snirf_file\": test_files,\n",
    "        \"trial_type\": test_labels})\n",
    "\n",
    "    # Save the test and train DataFrames to CSV files\n",
    "    test_df.to_csv(os.path.join(preprocessed_path, \"test_segments.csv\"), index=False)\n",
    "    train_df.to_csv(os.path.join(preprocessed_path, \"train_segments.csv\"), index=False)\n",
    "    return os.path.join(preprocessed_path, \"train_segments.csv\"), os.path.join(preprocessed_path, \"test_segments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4476cde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Excluding subjects: ['sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
      "Number of train subjects: 2\n",
      "Preloading 1980 trials into memory...\n",
      "Loaded 1980 trials into memory.\n",
      "Preloading 111 trials into memory...\n",
      "Loaded 111 trials into memory.\n",
      "Sub: sub-03, Epoch [1/400], Train Loss: 0.7776, Train Accuracy: 0.5616, Test Loss: 0.6918, Test Accuracy: 0.5225, Test F1 Score: 0.5225, Test F1 Avg: 0.5238\n",
      "Sub: sub-03, Epoch [2/400], Train Loss: 0.7815, Train Accuracy: 0.5561, Test Loss: 0.6902, Test Accuracy: 0.5045, Test F1 Score: 0.5045, Test F1 Avg: 0.5054\n",
      "Sub: sub-03, Epoch [3/400], Train Loss: 0.7830, Train Accuracy: 0.5485, Test Loss: 0.6924, Test Accuracy: 0.4775, Test F1 Score: 0.4775, Test F1 Avg: 0.4780\n",
      "Sub: sub-03, Epoch [4/400], Train Loss: 0.7752, Train Accuracy: 0.5414, Test Loss: 0.6840, Test Accuracy: 0.5045, Test F1 Score: 0.5045, Test F1 Avg: 0.5048\n",
      "Sub: sub-03, Epoch [5/400], Train Loss: 0.7889, Train Accuracy: 0.5566, Test Loss: 0.7074, Test Accuracy: 0.4865, Test F1 Score: 0.4865, Test F1 Avg: 0.4863\n",
      "Sub: sub-03, Epoch [6/400], Train Loss: 0.8006, Train Accuracy: 0.5646, Test Loss: 0.7065, Test Accuracy: 0.4595, Test F1 Score: 0.4595, Test F1 Avg: 0.4595\n",
      "Sub: sub-03, Epoch [7/400], Train Loss: 0.7805, Train Accuracy: 0.5571, Test Loss: 0.6959, Test Accuracy: 0.4775, Test F1 Score: 0.4775, Test F1 Avg: 0.4768\n",
      "Sub: sub-03, Epoch [8/400], Train Loss: 0.7892, Train Accuracy: 0.5586, Test Loss: 0.7016, Test Accuracy: 0.4865, Test F1 Score: 0.4865, Test F1 Avg: 0.4875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    141\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, criterion, optimizer, device)\n\u001b[0;32m--> 142\u001b[0m     _, train_accuracy, train_f1, train_f1_avg, train_acc_avg \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     test_loss, test_accuracy, test_f1, test_f1_avg, test_acc_avg \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion, device)\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# Store metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[47], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, criterion, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m acc_avg \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     32\u001b[0m         data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:93\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m---> 93\u001b[0m         clone[i] \u001b[38;5;241m=\u001b[39m \u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)([pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m data])  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57\u001b[0m, in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(data)   \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    f1_avg = []\n",
    "    acc_avg = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            f1_avg.append(f1_score(labels.cpu().numpy(), predicted.cpu().numpy(), average='micro'))\n",
    "            acc_avg.append((predicted == labels).sum().item() / labels.size(0))\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')  # or 'macro' if you prefer\n",
    "    return total_loss / len(test_loader), accuracy, f1, np.mean(f1_avg), np.mean(acc_avg)\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    num_epochs = 400\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 16\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load datasets\n",
    "    base_dir = \"/home\"\n",
    "    # dataset_path = os.path.join(base_dir, \"data/BallSqueezingHD_modified\")\n",
    "    # preprocessed_path = os.path.join(base_dir, \"data/yuanyuan_v2_processed_partial/\")\n",
    "    # preprocessed_path = os.path.join(\"datasets/processed/BallSqueezingHD_modified/\")\n",
    "    preprocessed_path = os.path.join(\"datasets/processed/FreshMotor/\")\n",
    "    \n",
    "    # subject_ids = ['sub-170', 'sub-173', 'sub-176', 'sub-179',\n",
    "    #             'sub-182', 'sub-577', 'sub-581', 'sub-586',  \n",
    "    #             'sub-613', 'sub-619', 'sub-633', 'sub-177',  \n",
    "    #             'sub-181', 'sub-183', 'sub-185', 'sub-568', \n",
    "    #             'sub-580', 'sub-583', 'sub-587', 'sub-592',  \n",
    "    #             'sub-618', 'sub-621', 'sub-638', 'sub-640']\n",
    "    \n",
    "    # subject_ids = ['sub-170', 'sub-173', 'sub-171', 'sub-174', 'sub-176', 'sub-179',\n",
    "    #             'sub-182', 'sub-177', 'sub-181', 'sub-183', 'sub-184', 'sub-185']\n",
    "\n",
    "    # subject_ids = ['sub-170', 'sub-173', 'sub-171']\n",
    "    subject_ids = ['sub-01', 'sub-02', 'sub-03']\n",
    "    # Parameters\n",
    "    k = 3\n",
    "    random_state = 42  # For reproducibility\n",
    "\n",
    "    # Shuffle the subject list\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    shuffled_subjects = rng.permutation(subject_ids)\n",
    "\n",
    "    # Split into k roughly equal folds\n",
    "    folds = np.array_split(shuffled_subjects, k)\n",
    "\n",
    "    # Optional: convert each fold to a list\n",
    "    folds = [list(fold) for fold in folds]\n",
    "\n",
    " \n",
    "    # exclude_subjects = ['sub-547', 'sub-639', 'sub-588', 'sub-171', 'sub-174', 'sub-184']\n",
    "    # exclude_subjects = ['sub-547', 'sub-639', 'sub-588']\n",
    "    exclude_subjects = ['sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\n",
    "    # exclude_subjects = [ 'sub-174', 'sub-176', 'sub-179', 'sub-182', 'sub-177', 'sub-181', 'sub-183', 'sub-184', 'sub-185']\n",
    "    \n",
    "    chromo = \"yuanyuan_v2\"\n",
    "    for fold in folds:\n",
    "        subs = \"_\".join(fold)\n",
    "\n",
    "        # Device configuration\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "\n",
    "        train_csv_path, test_csv_path = create_train_test_segments(\n",
    "            None,\n",
    "            preprocessed_path, \n",
    "            test_subjects_list=fold, \n",
    "            exclude_subjects=exclude_subjects\n",
    "        )\n",
    "        train_csv = pd.read_csv(train_csv_path)\n",
    "        test_csv = pd.read_csv(test_csv_path)\n",
    "\n",
    "        train_dataset = fNIRSPreloadDataset(\n",
    "            train_csv_path, chromo='HbO')\n",
    "        test_dataset = fNIRSPreloadDataset(\n",
    "            test_csv_path, mode=\"test\", chromo='HbO')\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        # Initialize model, loss, and optimizer\n",
    "        model = CNN2DImage().to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        test_f1_avgs = []\n",
    "        test_f1s = []\n",
    "        train_f1_avgs = []\n",
    "        train_f1s = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "            _, train_accuracy, train_f1, train_f1_avg, train_acc_avg = evaluate_model(model, train_loader, criterion, device)\n",
    "            test_loss, test_accuracy, test_f1, test_f1_avg, test_acc_avg = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "\n",
    "            # Store metrics\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_f1_avgs.append(test_f1_avg)\n",
    "            test_f1s.append(test_f1)\n",
    "            train_f1_avgs.append(train_f1_avg)\n",
    "            train_f1s.append(train_f1)\n",
    "\n",
    "            print(f\"Sub: {subs}, Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "                f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, \"\n",
    "                f\"Test F1 Avg: {test_f1_avg:.4f}\")\n",
    "       \n",
    "        res = {\"train_loss\": train_losses, \"train_accuracy\": train_accuracies,\n",
    "                \"test_loss\": test_losses, \"test_accuracy\": test_accuracies, \"test_f1\": test_f1s,\n",
    "                \"test_f1_avg\": test_f1_avgs, \"test_acc_avg\": test_acc_avg,\n",
    "                \"train_f1\": train_f1s, \"train_f1_avg\": train_f1_avgs, \"train_acc_avg\": train_acc_avg}\n",
    "        with open(f\"/home/results/res_{subs}_{chromo}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(res, f)\n",
    "\n",
    "        # Save the trained model\n",
    "        torch.save(model.state_dict(), f\"/home/checkpoints/model_{subs}_{chromo}.pth\")\n",
    "        print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5492c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b224ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
